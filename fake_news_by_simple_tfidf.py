# -*- coding: utf-8 -*-
"""fake news by simple tfidf.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dzUxKaeqYuWHgb0J4t9Q5LH0Zc0qXWCa
"""

import pandas as pd

# Load the CSV files
gossipcop_fake = pd.read_csv('gossipcop_fake.csv')
gossipcop_real = pd.read_csv('gossipcop_real.csv')
politifact_fake = pd.read_csv('politifact_fake.csv')
politifact_real = pd.read_csv('politifact_real.csv')

# Assign labels based on filenames
gossipcop_fake['label'] = 0
gossipcop_real['label'] = 1
politifact_fake['label'] = 0
politifact_real['label'] = 1

# Concatenate all DataFrames
combined_data = pd.concat([gossipcop_fake, gossipcop_real, politifact_fake, politifact_real], ignore_index=True)

# Optionally, you can drop any unnecessary columns like 'id', 'news_url', 'tweet_ids' if they are not needed for modeling
combined_data = combined_data[['title', 'label']]

# Optionally, you can preprocess the titles if needed

# Save the combined dataset to a new CSV file
combined_data.to_csv('combined_news_dataset.csv', index=False)

print(f"Combined dataset shape: {combined_data.shape}")
print(combined_data['label'].value_counts())



import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import string

# Download necessary NLTK data
nltk.download('punkt')
nltk.download('stopwords')



data = pd.read_csv('/content/combined_news_dataset.csv')

data.columns

# Preprocess the text
def preprocess_text(text):
    # Tokenize the text
    tokens = word_tokenize(text)
    # Convert to lower case
    tokens = [word.lower() for word in tokens]
    # Remove punctuation
    tokens = [word for word in tokens if word.isalpha()]
    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    tokens = [word for word in tokens if not word in stop_words]
    # Join the tokens back into a single string
    return ' '.join(tokens)

data['titale'] = data['title'].apply(preprocess_text)

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(data['titale'], data['label'], test_size=0.2, random_state=42)

# Vectorize the text using TF-IDF
vectorizer = TfidfVectorizer(max_features=5000)
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

# Train a Logistic Regression model
model = LogisticRegression()
model.fit(X_train_tfidf, y_train)

# Make predictions
y_pred = model.predict(X_test_tfidf)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print(f'Accuracy: {accuracy}')
print(f'Precision: {precision}')
print(f'Recall: {recall}')
print(f'F1 Score: {f1}')

# Example of testing the model with new text input (optional)
def test_model(input_text, model, vectorizer):
    preprocessed_text = preprocess_text(input_text)
    input_vectorized = vectorizer.transform([preprocessed_text])
    prediction = model.predict(input_vectorized)
    return 'Fake News' if prediction[0] == 0 else 'Real News'

# Example usage:
new_text = "us attaked by bomb"
prediction = test_model(new_text, model, vectorizer)
print(f'Prediction for "{new_text}": {prediction}')